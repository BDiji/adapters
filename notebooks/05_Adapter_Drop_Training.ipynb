{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iYLQO5Evvqy"
   },
   "source": [
    "# Training a `Robust' Adapter with AdapterDrop\n",
    "\n",
    "This notebook extends our quickstart adapter training notebook to illustrate how we can use AdapterDrop\n",
    "to robustly train an adapter, i.e. adapters that allow us to dynmically dropp layers for faster multi-task inference.\n",
    "Please have a look at the original adapter training notebook for more details on the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-XTIOLv0isn"
   },
   "source": [
    "## Installation\n",
    "\n",
    "First, let's install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ju-alwbHmKYA",
    "outputId": "44bbe24a-0925-46c2-aaf8-d9b5cdd7d0e3"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq adapters\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Mx916lBCfoL",
    "outputId": "bec39206-a7d9-415a-a3fa-90b948d51489",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3920852a16c4115abffc5597e6aa407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "dataset = dataset.map(encode_batch, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2-2CbfPGYvi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tp9uG-pT-qgv",
    "outputId": "c6a19f25-26b9-44c7-887c-1d1a71e957fa"
   },
   "outputs": [],
   "source": [
    "from adapters import RobertaAdapterModel\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"👎\", 1: \"👍\"},\n",
    ")\n",
    "model = RobertaAdapterModel.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"rotten_tomatoes\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\"rotten_tomatoes\", num_labels=2)\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev5t_8i8HzJB"
   },
   "source": [
    "To dynamically drop adapter layers during training, we make use of HuggingFace's `TrainerCallback'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FRft_5AAlQd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from adapters import AdapterTrainer\n",
    "from transformers import TrainingArguments, EvalPrediction, TrainerCallback\n",
    "\n",
    "class AdapterDropTrainerCallback(TrainerCallback):\n",
    "  def on_step_begin(self, args, state, control, **kwargs):\n",
    "    skip_layers = list(range(np.random.randint(0, 11)))\n",
    "    kwargs['model'].set_active_adapters(\"rotten_tomatoes\", skip_layers=skip_layers)\n",
    "\n",
    "  def on_evaluate(self, args, state, control, **kwargs):\n",
    "    # Deactivate skipping layers during evaluation (otherwise it would use the\n",
    "    # previous randomly chosen skip_layers and thus yield results not comparable\n",
    "    # across different epochs)\n",
    "    kwargs['model'].set_active_adapters(\"rotten_tomatoes\", skip_layers=None)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")\n",
    "\n",
    "trainer.add_callback(AdapterDropTrainerCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iHhoYuLIdX3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now train and evaluate our robustly trained adapter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "zZxaujENntNR",
    "outputId": "6700e4ac-1258-4bd8-ac30-a578c8d6c4ba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='1602' max='1602' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1602/1602 07:38, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='34' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 6.0,\n",
       " 'eval_acc': 0.8799249530956847,\n",
       " 'eval_loss': 0.29594820737838745,\n",
       " 'eval_mem_cpu_alloc_delta': 112111,\n",
       " 'eval_mem_cpu_peaked_delta': 214624,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_gpu_peaked_delta': 94487040,\n",
       " 'eval_runtime': 5.0862,\n",
       " 'eval_samples_per_second': 209.586}"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Adapter_Drop_Training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
