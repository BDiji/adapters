{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG7kC-qt4C99"
   },
   "source": [
    "# Load Head with id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "BDbnBJQ64ByT",
    "outputId": "2ecdcdc1-e88e-437b-f47c-99d4d3944317"
   },
   "outputs": [],
   "source": [
    "!pip install -Uq adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zd8fsYXX4uxm"
   },
   "source": [
    "First, we create the model and load the trained adapter and head. With the head, the labels, and the i2label mapping is automatically loaded. With `model.get_labels()` and `model.get_labels_dict()` you can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uGJQZPz4QNz",
    "outputId": "580de827-4766-4fca-9bbf-8ff2ee6288d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertAdapterModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n",
      "{1: 'B-LOC', 7: 'B-MISC', 5: 'B-ORG', 3: 'B-PER', 2: 'I-LOC', 8: 'I-MISC', 6: 'I-ORG', 4: 'I-PER', 0: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, AdapterConfigBase\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoAdapterModel.from_pretrained(model_name)\n",
    "adapter_name = model.load_adapter(\"ner/conll2003@ukp\")\n",
    "\n",
    "model.active_adapters = adapter_name\n",
    "model.active_head = \"ner_head\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# How you can acces the labels and the mapping for a pretrained head\n",
    "print(model.get_labels())\n",
    "print(model.get_labels_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-Ps9GKJ5ahe"
   },
   "source": [
    "This helper function allows us to get the sequence of ids of the predicted output for a specific input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ylUZiWHs6Oay"
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "  tokens = tokenizer.encode(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "  model.eval()\n",
    "  preds = model(tokens, adapter_names=['ner'])[0]\n",
    "  preds = preds.detach().numpy()\n",
    "  preds = np.argmax(preds, axis=2)\n",
    "  return tokenizer.tokenize(sentence), preds.squeeze()[1:-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuW4eYx5WJE9"
   },
   "source": [
    "If we want to use the model to predict the labels of a sentence we can use the `model.get_labels_dict()` function to map the predicted label ids to the corresponding label, as for the example text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-q1ErWcF5Z2w",
    "outputId": "baa59a26-2ba5-4ec3-fd6c-6a326a3c0ec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germany(B-LOC) '(O) s(O) representative(O) to(O) the(O) european(B-ORG) union(I-ORG) '(O) s(O) veterinary(B-ORG) committee(I-ORG) werner(B-PER) z(I-PER) ##wing(I-PER) ##mann(I-PER) said(O) on(O) wednesday(O) consumers(O) should(O) buy(O) sheep(O) ##me(O) ##at(O) from(O) countries(O) other(O) than(O) britain(B-LOC) until(O) the(O) scientific(O) advice(O) was(O) clearer(O) .(O) "
     ]
    }
   ],
   "source": [
    "example_text=\"Germany's representative to the European Union\\'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer.\"\n",
    "# get the mapping of ids to labels\n",
    "label_map = model.get_labels_dict()\n",
    "tokens, preds = predict(example_text)\n",
    "for token, pred in zip(tokens, preds):\n",
    "  print(f\"{token}({label_map[pred]}) \", end=\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "load_id2label.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
